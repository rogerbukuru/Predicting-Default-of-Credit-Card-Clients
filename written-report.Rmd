---
title: "Predicting default of credit card clients"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: true
      df_print: kable
      fig_width: 5
      fig_height: 5
      fig_caption: true
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Import Data 

```{r, echo = FALSE}
rm(list = ls())
library(readxl)
library(tidyverse)
library(knitr)
library(ggplot2)
library(dplyr)
library(psych)

file_path = "./data/default of credit card clients.xls"

credit_default_data = as_tibble(read_xls(file_path))

colnames(credit_default_data) = as.character(credit_default_data[1, ])
                                
credit_default_data = credit_default_data[-1,]
credit_default_data = credit_default_data[,-1]

```


# Exploratory Data Analysis

## Describe Data

```{r, table_figure, fig.cap="Table 1: Example Table", echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))

credit_default_data = credit_default_data %>%
                      mutate(across(where(is.character), as.numeric))
credit_default_data_2 = credit_default_data
#colnames(credit_default_data_summary)
#summary(credit_default_data)
stats = describe(credit_default_data, na.rm = FALSE, skew= FALSE,  check=TRUE, omit = TRUE)
#round(stats,2)
kable(round(stats,2), caption = "Features Summary Statistics")
```
- Education and Marriage have some unreported categories

## Response Variable Distribution

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", "credit_default_data_2")
rm(list = setdiff(ls(), objects_to_keep))
response_variable = as.numeric(credit_default_data$`default payment next month`) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes = response_variable %>%
              filter(Class == 1)


default_no =  response_variable %>%
              filter(Class == 0)

agg_data = response_variable %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes = nrow(default_yes)/nrow(credit_default_data) *100
percentage_no = nrow(default_no)/nrow(credit_default_data) *100  

ggplot(agg_data, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend
```


## Data Cleaning


```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data","credit_default_data_2")
rm(list = setdiff(ls(), objects_to_keep))

# Marriage and Education and to other cateogry
credit_default_data = credit_default_data %>%
                      rename(PAY_1 = PAY_0)%>% 
                      rename(DEFAULT_PAYMENT_NEXT_MONTH = `default payment next month`)

marriage_cateogory_codes = c(1,2,3)   # From data description
education_category_codes = c(1,2,3,4) # From data description

marriage_stats = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats

education_stats = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats

# Add unknown category to the 'Other' category for both Marriage and Education 
credit_default_data  = credit_default_data %>%
                       mutate(MARRIAGE = if_else(!(MARRIAGE %in% marriage_cateogory_codes), 3, MARRIAGE),
                       EDUCATION = if_else(!(EDUCATION %in% education_category_codes), 4, EDUCATION)
                       )
        

marriage_stats_2 = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats_2

education_stats_2 = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats_2


# Payment History where value is -2 and -1, aggregate under 0

payment_history = c("PAY_1", "PAY_2", "PAY_3", "PAY_4", "PAY_5","PAY_6")

for (p in payment_history) {
  indices = which(credit_default_data[,p] < 0)
  credit_default_data[indices, p] = 0
}
```


## Data Analysis

```{r, echo = FALSE}

objects_to_keep <- c("credit_default_data","credit_default_data_2" )
categorical_feature_names = c("SEX", "EDUCATION", "MARRIAGE", "DEFAULT_PAYMENT_NEXT_MONTH")
credit_default_data = credit_default_data%>%
                      mutate(across(matches(categorical_feature_names), as.factor))

stats_2 = describe(credit_default_data, na.rm = FALSE, skew= FALSE, omit = TRUE)
#round(stats,2)
kable(round(stats_2,2), caption = "Features Summary Statistics")

rm(list = setdiff(ls(), objects_to_keep))
limit_balances  = credit_default_data |>
                  drop_na(LIMIT_BAL) |>
                  select(LIMIT_BAL) |>
                  as.matrix() |>
                  as.vector() |>
                  as.numeric()


average_limit_balance = mean(limit_balances)
range(limit_balances)

male_average_credit_limit = credit_default_data |>
                            filter(SEX == 1) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

female_average_credit_limit = credit_default_data |>
                            filter(SEX == 2) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

minimum = min(limit_balances)
first_quantile = quantile(limit_balances, 0.25)
median_point = quantile(limit_balances, 0.50)
third_quantile = quantile(limit_balances, 0.75)
maximum = max(limit_balances)

ggplot(credit_default_data, aes(x="", y=limit_balances)) +
  geom_boxplot(fill="lightblue", color="black") +
  labs(title="Credit Limit Five Number Summary", x="", y="Credit Limit") +
  theme_minimal()


# Credit Spending
credit_spending = credit_default_data %>%
                  select(LIMIT_BAL, BILL_AMT1) %>%
                  mutate(across(everything(), as.numeric)) %>%
                  mutate(`Credit Utilization` = BILL_AMT1) %>%
                  ungroup()
credit_spending = credit_spending %>%
                  mutate(`Percentage Utilized`= round((`Credit Utilization`/LIMIT_BAL)*100,2))


ggplot(credit_spending, aes(x = `Percentage Utilized`)) +
  geom_histogram(aes(y=..density..),binwidth = 5, # Adjust binwidth based on your data's distribution and scale
                 fill = "steelblue", color = "black") +
  xlim(0, 200)+
  labs(title = "Distribution of Credit Utilization",
       x = "Credit Utilization (%)",
       y = "# of Customers") +
  theme_minimal() 
```

## Normality Check

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data","credit_default_data_2" )
rm(list = setdiff(ls(), objects_to_keep))
library(MVN)
#mvn_result <- mvn(data = train_X_continous, mvnTest = "mardia")
#print(mvn_result)

#shapiro.test(train_X_continous$LIMIT_BAL[1:5000])
```


## Correlation Analysis

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data",
                     "credit_default_data_2",
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(corrplot)

#summary(X_categorical)

# corr_matrix = cor(train_X_continous)
# col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# corrplot(corr_matrix, method = "color", col = col(200), 
#          type = "full", order = "original", 
#          addCoef.col = "white",
#          tl.col = "black", tl.srt = 45,
#          tl.pos = "lt",
#          tl.cex = 0.6, cl.cex = 0.7,
#          number.cex = 0.5
#          )

# Standardize Data

X_continous = credit_default_data%>%
              select(where(is.numeric)) %>%
              mutate(DEFAULT = as.numeric(as.matrix(credit_default_data[, 24])))
              
corr_matrix = cor(as.matrix(X_continous))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corr_matrix, method = "color", col = col(200),
        type = "full", order = "original",
        addCoef.col = "white",
        tl.col = "black", tl.srt = 45,
       tl.pos = "lt",
       tl.cex = 0.6, cl.cex = 0.7,
        number.cex = 0.5
       )
```

We observe from the correlation matrix, that some features have high correlations with each other

- BILL_AMT1 and BILL_AMT2 have  p = 0.95
- BILL_AMT2 and BILL_AMT3 have  p = 0.92
- BILL_AMT4 and BILL_AMT5 have  p = 0.94


# Data Preprocessing


## One-Hot Encoding

```{r, echo = FALSE}
library(caret)

dummies = dummyVars("~.", data = credit_default_data[, -24], fullRank = FALSE)

data_transformed <- predict(dummies, newdata = credit_default_data[, -24])%>%as.tibble()
data_transformed$DEFAULT_PAYMENT_NEXT_MONTH = as.factor(as.vector(as.matrix(credit_default_data[, 24])))
credit_default_data = data_transformed%>%
                      select(-SEX.2, -EDUCATION.4, -MARRIAGE.3) # Remove columns to ensure full rank

```

## Dataset Partition

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", "X_continous")
rm(list = setdiff(ls(), objects_to_keep))
#summary(credit_default_data)

# Split Data
set.seed(10032024)
dataSize = nrow(credit_default_data)
trainingSize = floor(0.80*dataSize)



trainingDataIndices = sample(seq_len(dataSize), size = trainingSize)
training_data = credit_default_data[trainingDataIndices,]
testing_data  = credit_default_data[-trainingDataIndices,]

# Class Distribution of Train and Test Data Set

training_data_response = training_data%>%
                         select(DEFAULT_PAYMENT_NEXT_MONTH)

training_data_defaulters = training_data_response %>%
                           filter(DEFAULT_PAYMENT_NEXT_MONTH == 1)

training_data_non_defaulters = training_data_response %>%
                               filter(DEFAULT_PAYMENT_NEXT_MONTH == 0)

training_data_agg = training_data_response %>%
                   group_by(DEFAULT_PAYMENT_NEXT_MONTH) %>%
                   summarise(Count = n()) %>%
                   mutate(Percentage = Count /sum(Count) *100)


ggplot(training_data_agg, aes(x = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")), y = Count, fill = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Training Data Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend



testing_data_response = testing_data%>%
                        select(DEFAULT_PAYMENT_NEXT_MONTH)

testing_data_defaulters = testing_data_response %>%
                           filter(DEFAULT_PAYMENT_NEXT_MONTH == 1)

testing_data_non_defaulters = testing_data_response %>%
                               filter(DEFAULT_PAYMENT_NEXT_MONTH == 0)

testing_data_agg = testing_data_response %>%
                   group_by(DEFAULT_PAYMENT_NEXT_MONTH) %>%
                   summarise(Count = n()) %>%
                   mutate(Percentage = Count /sum(Count) *100)


ggplot(testing_data_agg, aes(x = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")), y = Count, fill = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Testing Data Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend

training_data_total_defaulters = nrow(training_data_defaulters)
training_data_total_non_defaulters = nrow(training_data_non_defaulters)

testing_data_total_defaulters = nrow(testing_data_defaulters)
testing_data_total_non_defaulters = nrow(testing_data_non_defaulters)
                                
```
## Feature Scaling

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", "training_data", "testing_data")
rm(list = setdiff(ls(), objects_to_keep))
# Standardization
training_data_x = training_data[, -27]

mu_training_data = apply(training_data_x, 2, mean)
sigma_training_data = apply(training_data_x, 2, sd)

training_data_x_std = as.matrix(apply(training_data_x, 2, function(training_data_x)((training_data_x - mean(training_data_x))/sd(training_data_x))) %>% as.tibble())

testing_data_x = testing_data[, -27]

mu_testing_data = apply(testing_data_x, 2, mean)
sigma_testing_data = apply(testing_data_x, 2, sd)

testing_data_x_std = as.matrix(apply(testing_data_x, 2, function(testing_data_x)((testing_data_x - mean(testing_data_x))/sd(testing_data_x))) %>% as.tibble())
```



## Resampling

#### Oversampling

```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std"
                     )
rm(list = setdiff(ls(), objects_to_keep))
#library(smotefamily)
#install.packages(c("zoo","xts","quantmod", "ROCR")) ## and perhaps mode

#install.packages("https://cran.r-project.org/src/contrib/Archive/DMwR/DMwR_0.4.1.tar.gz", repos=NULL, type="source")
library(DMwR)

# Assume your target variable is binary and the minority class is "1"
#result <- SMOTE(form = target_variable ~ ., dat = your_data, perc.over = 100, k = 5)

training_data_smote <- SMOTE(DEFAULT_PAYMENT_NEXT_MONTH ~ ., data = as.data.frame(training_data), perc.over = 200, k = 5)

training_data_response_smote = training_data_smote%>%
                              select(DEFAULT_PAYMENT_NEXT_MONTH)

training_data_defaulters_smote = training_data_response_smote %>%
                           filter(DEFAULT_PAYMENT_NEXT_MONTH == 1)

training_data_non_defaulters_smote = training_data_response_smote %>%
                               filter(DEFAULT_PAYMENT_NEXT_MONTH == 0)

training_data_agg_smote = training_data_response_smote %>%
                   group_by(DEFAULT_PAYMENT_NEXT_MONTH) %>%
                   summarise(Count = n()) %>%
                   mutate(Percentage = Count /sum(Count) *100)


ggplot(training_data_agg_smote, aes(x = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")), y = Count, fill = factor(DEFAULT_PAYMENT_NEXT_MONTH, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Training Data Class Distribution after SMOTE") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend


training_data_x_smote = training_data_smote[, -27]

mu_training_data_smote = apply(training_data_x_smote, 2, mean)
sigma_training_data_smote = apply(training_data_x_smote, 2, sd)

training_data_x_std_smote = as.matrix(apply(training_data_x_smote, 2, function(training_data_x_smote)((training_data_x_smote - mean(training_data_x_smote))/sd(training_data_x_smote))) %>% as.tibble())
```

# Feature Engineering

## Feature Extraction

### Principal Component Analysis

```{r, echo= FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std",
                     "training_data_smote",
                     "training_data_x_smote",
                     "training_data_x_std_smote",
                     "mu_training_data_smote",
                     "sigma_training_data_smote"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(ggplot2)
library(scatterplot3d)
library(plotly)
  
sigma_matrix   = cov(training_data_x_std_smote)
eigen_decomp   = eigen(sigma_matrix)

eigenvalues   = eigen_decomp$values 
eigenvectors  = as.matrix(eigen_decomp$vectors)
colnames(eigenvectors) = paste("PC", 1:nrow(sigma_matrix), sep = "")


total_variance = sum(eigenvalues)
variance_explained = eigenvalues/total_variance
pca_data = data.frame(PCA = 1:nrow(sigma_matrix), `Variance Explained` = variance_explained)

# Scree Plot

ggplot(pca_data, mapping = aes(x = PCA, y = Variance.Explained)) +
      geom_point() +
      geom_line()+
      labs(x = "Principal Components", y = "Variance Explained", title = "PCA Scree Plot")+
      theme_minimal()
     

# Suggest that the first 4 PCA
var_first_11 = sum(variance_explained[1:14])

variance_explained

pca_scores =  t(tcrossprod(eigenvectors, training_data_x_std_smote)) %>% as.tibble()
colnames(pca_scores) = colnames(eigenvectors)

# Perform clustering on the latent space representations
    num_clusters <- 5  # Specify the desired number of clusters
    kmeans_result <- kmeans(pca_scores, centers = num_clusters)
    # Assign colors to each cluster
    cluster_colors <- c("#0799BC", "darkgreen", "#F98524", "#F1EA81","#222222")  # Specify colors for each cluster
    point_colors <- cluster_colors[kmeans_result$cluster]
scatterplot3d(x = pca_scores$PC1, y = pca_scores$PC2, z = pca_scores$PC3, main = "Principal Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = point_colors, cex.axis = 0.5)
```

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std",
                     "training_data_smote",
                     "training_data_x_smote",
                     "training_data_x_std_smote",
                     "mu_training_data_smote",
                     "sigma_training_data_smote",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))

classic_pca = prcomp(training_data_x_std_smote, retx = TRUE, center = FALSE, scale. = FALSE)
eigenvalues = classic_pca$sdev^2
total_variance = sum(eigenvalues)
variance_explained = eigenvalues/total_variance
pca_data = data.frame(PCA = 1:nrow(classic_pca$rotation), `Variance Explained` = variance_explained)

# Suggest that the first 4 PCA
var_first_11 = sum(variance_explained[1:11])

var_first_11
# Scree Plot
ggplot(pca_data, mapping = aes(x = PCA, y = Variance.Explained)) +
      geom_point() +
      geom_line()

    num_clusters <- 5  # Specify the desired number of clusters
    kmeans_result <- kmeans(pca_scores, centers = num_clusters)
    # Assign colors to each cluster
    cluster_colors <- c("#0799BC", "darkgreen", "#F98524", "#F1EA81","#222222")  # Specify colors for each cluster
    point_colors <- cluster_colors[kmeans_result$cluster]
scatterplot3d(x = classic_pca$x[,1], y = classic_pca$x[,2], z = classic_pca$x[,3], main = "Principal Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = point_colors, cex.axis = 0.5)
```


### Autoencoders


```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std",
                     "training_data_smote",
                     "training_data_x_smote",
                     "training_data_x_std_smote",
                     "mu_training_data_smote",
                     "sigma_training_data_smote",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))


library(keras)

train_autoencoder = function(epochs = 100, code_size = 1, x_train) {
  
total_features = ncol(x_train)
input_layer = layer_input(shape = c(total_features))

encoder = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "tanh") %>%# encoder layer 1
  #layer_batch_normalization() %>%  # Batch normalization layer to normalize the activations of the input layer
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = total_features/2, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = code_size) # Botteneck layer/ Laten Layer
  
decoder = 
  encoder %>%
  layer_dense(units = total_features/2 , activation = "tanh") %>% # Decoder Layer 1
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = 15, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = total_features) # Reconstructed/Output layer

  
autoencoder_model = keras_model(inputs = input_layer, outputs = decoder)  
autoencoder_model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("accuracy")
)

summary(autoencoder_model)

  auto_associative =
  autoencoder_model %>%
  fit(as.matrix(x_train), 
             as.matrix(x_train),
             epochs = epochs,
             batch_size = 256,
             shuffle = TRUE,
             validation_split = 0.2
             #validation_data = list(as.matrix(validation_X_continous_std), as.matrix(validation_X_continous_std))
             )
  plot(auto_associative)
  if(code_size >= 1 & code_size <=3){
  visualize_compressed_data(autoencoder_model,input_layer, encoder, code_size,x_train)
  }
  return(list(aem = autoencoder_model))
}

visualize_compressed_data = function(autoencoder_model, input_layer, encoder, code_size = 1, x_train){
  
if(code_size >3){
  return(NULL)
}
autoencoder_weights = 
  autoencoder_model %>%
  get_weights()

file_name = paste("autoencoder_weights", code_size, sep = "-")
save_model_weights_hdf5(object = autoencoder_model,filepath = paste0("./data/", file_name, ".hdf5"),overwrite = TRUE)

encoder_model <- keras_model(inputs = input_layer, outputs = encoder)

encoder_model %>% load_model_weights_hdf5(filepath = paste0("./data/", file_name, ".hdf5"),skip_mismatch = TRUE,by_name = TRUE)

encoder_model %>% compile(
  loss='mean_squared_error',
  optimizer='adam',
  metrics = c('accuracy')
)

# Visualize how model has embedded data in 3-dimensions

embedded_points = encoder_model %>%
                  predict_on_batch(x = as.matrix(x_train)) %>%
                  as.tibble()
#plot(embedded_points$V1, embedded_points$V2)
  if(code_size == 1){
    plot(embedded_points$V1)
  }else if(code_size == 2){
        plot(embedded_points$V1, embedded_points$V2)
    
  }else if(code_size == 3){
       # Perform clustering on the latent space representations
    num_clusters <- 5  # Specify the desired number of clusters
    kmeans_result <- kmeans(embedded_points, centers = num_clusters)
    # Assign colors to each cluster
    cluster_colors <- c("#0799BC", "darkgreen", "#F98524", "#F1EA81","#222222")  # Specify colors for each cluster
    point_colors <- cluster_colors[kmeans_result$cluster]
    
     scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = point_colors, cex.axis = 0.5)
  }else {
    print("Cannot visualize higher than 3-D")
  }
}
autoencoder_mse = rep(NA, 5)

for(k in 1:1){
  train_aem = train_autoencoder(epochs = 100, code_size = 3, training_data_x_std_smote)
  autoencoder_stats = evaluate(train_aem$aem,as.matrix(training_data_x_std_smote),as.matrix(training_data_x_std_smote))
  autoencoder_mse[k] = autoencoder_stats["loss"]
}

autoencoder_mse

```

### Polynomial PCA

```{r, echo=FALSE}

# # Create polynomial features
# df_poly <- train_X_continous_std_oversampled %>%as.tibble()%>%
#   mutate(across(everything(), list(
#     squared = ~ .^2
#   )))
# 
# classic = prcomp(df_poly, retx = TRUE, center = FALSE, scale. = FALSE)
# 
# 
# summary(classic)
# #classic$rotation == eigenvectors
# 
# #classic$x
# 
# scatterplot3d(x = classic$rotation[,1], y = classic$rotation[,2], z = classic$rotation[,3], main = "Principal Components",xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)

```


### Variational Autoencoders

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std",
                     "training_data_smote",
                     "training_data_x_smote",
                     "training_data_x_std_smote",
                     "mu_training_data_smote",
                     "sigma_training_data_smote",
                     "pca_scores",
                     "eigenvectors",
                     "autoencoder_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))

sampling_layer <- new_layer_class(
  classname = "SamplingLayer",
  initialize = function(latent_dim) {
    super()$`__init__`()
    self$latent_dim <- as.integer(latent_dim)
    self$epsilon <- k_random_normal(shape = c(1, latent_dim), mean = 0, stddev = 1)  # Pre-generate noise
  },
  call = function(self, inputs) {
    z_mean <- inputs[[1]]
    z_log_var <- inputs[[2]]
    z = z_mean + k_exp(z_log_var / 2) * self$epsilon
    
    kl_loss <- -0.5 * k_sum(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1)
    kl_loss <- k_mean(kl_loss) # Take the mean of the KL divergence loss
    self$add_loss(kl_loss)
    
    return(z)
  }
)

variational_autoencoder = function(latent_space_dim,batch_size, x_train, epochs, num_clusters){
  
total_features = ncol(x_train)
input_layer = layer_input(shape = c(total_features), name="inputLayer")

encoder_inputs = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "tanh", name = "encoderHiddenL1")#Hidden layer 1


# Latent Space
z_mean = encoder_inputs %>% layer_dense(units = latent_space_dim, name = "z_mean")
z_log_var = encoder_inputs %>% layer_dense(units = latent_space_dim, name= "z_log_var")

sampling <- sampling_layer(latent_dim = latent_space_dim)
z <- sampling(list(z_mean, z_log_var))

encoder = keras_model(inputs = input_layer, outputs = list(z_mean, z_log_var,z), name = "encoder")
summary(encoder)


decoder_hidden_l1 = layer_dense(units = total_features/2 , activation = "tanh", name="decoderHiddenL1") #Hidden layer 1
decoder_output = layer_dense(units = total_features, activation = "tanh", name = "decoderOutput") # Reconstructed/Output layer

sample_decode_h_l1 = decoder_hidden_l1(z)
sample_final_decode = decoder_output(sample_decode_h_l1)
vae_model = keras_model(inputs = input_layer, outputs = sample_final_decode, name = "vae_model")
summary(vae_model)

vae_loss = function(x, vae_output){
  reconstruction_loss <- k_mean(k_square(x - vae_output), axis = -1) * latent_space_dim
  kl_loss <- -0.5 * k_sum(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1)
  k_mean(reconstruction_loss + kl_loss)
}

# Custom loss function
 reconstruction_loss <- function(x, vae_output) {
    k_mean(k_square(x - vae_output), axis = -1)
  }

vae_model %>% compile(loss = reconstruction_loss, optimizer = "adam", metrics = c("accuracy", "mse"))


vae = vae_model %>% fit(x_train, x_train,
            epochs = epochs, batch_size = batch_size,
            validation_split = 0.2
            #validation_data = list(as.matrix(x_validation), as.matrix(x_validation))
            )
plot(vae)
visual_vae_latent_data(latent_space_dim,input_layer, z, x_train, vae_model)
return(list(vae_model = vae_model))
}

visual_vae_latent_data = function(latent_space_dim,input_layer, z, x_train, vae_model){
  if(latent_space_dim >3){
  return(NULL)
}
  vae_weights = 
  vae_model %>%
  get_weights()
  
  file_name = paste("vae_weights", latent_space_dim, sep = "-")
  save_model_weights_hdf5(object = vae_model,filepath = paste0("./data/", file_name, ".hdf5"),overwrite = TRUE)
  
  encoder_model   = keras_model(inputs = input_layer, outputs = z, name = "encoder_model")
  encoder_model %>% load_model_weights_hdf5(filepath = paste0("./data/", file_name, ".hdf5"),skip_mismatch = TRUE,by_name = TRUE)
  
  embedded_points = predict(encoder_model, x_train)%>% as.tibble()
  if(latent_space_dim == 1){
    plot(embedded_points$V1)
  }else if(latent_space_dim == 2){
        plot(embedded_points$V1, embedded_points$V2)
    
  }else if(latent_space_dim == 3){
    # Perform clustering on the latent space representations
    num_clusters <- 5  # Specify the desired number of clusters
    kmeans_result <- kmeans(embedded_points, centers = num_clusters)
    # Assign colors to each cluster
    cluster_colors <- c("#0799BC", "darkgreen", "#F98524", "#F1EA81","#222222")  # Specify colors for each cluster
    point_colors <- cluster_colors[kmeans_result$cluster]
    
     scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Variational Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = point_colors, cex.axis = 0.5)
  }else {
    print("Cannot visualize higher than 3-D")
  }
}



vae_mse = rep(NA, 5)

for(k in 1:1){
 train_vae_model = variational_autoencoder(
                        latent_space_dim = 3,
                        batch_size = 256,
                        x_train = as.matrix(training_data_x_std_smote),
                        epochs = 100
                        )
 vae_stats = evaluate(
   train_vae_model$vae_model,
   as.matrix(training_data_x_std_smote),
   as.matrix(training_data_x_std_smote)
   )
 vae_mse[k] = vae_stats["loss"]
}

vae_mse

```


### Technique evaluation

```{r, echo=FALSE}

objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "testing_data",
                     "training_data_x",
                     "training_data_x_std",
                     "testing_data_x",
                     "testing_data_x_std",
                     "training_data_smote",
                     "training_data_x_smote",
                     "training_data_x_std_smote",
                     "mu_training_data_smote",
                     "sigma_training_data_smote",
                     "pca_scores",
                     "eigenvectors",
                     "autoencoder_mse",
                     "vae_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))

pca_stats = function(pca, x, x_std, mu_x, sigma_x, k){
x_projected =  x_std%*%pca[, 1:k]%>%as.tibble()
#x_projected_og_scale = sweep(x_projected,2,sigma_x, "*")
#x_projected_og_scale = sweep(x_projected,2,mu_x, "+")
mse = mean(as.matrix((as.matrix(x_std)-x_projected)^2))
return(list(x = x_projected, mse = mse))
}

pca_mse = rep(NA, 20)
for(k in 1:26){
 pca_mse[k] = pca_stats(eigenvectors,training_data_x_std_smote, training_data_x_std_smote, mu_training_data_smote,     sigma_training_data_smote, k)$mse

}



df <- data.frame(k = c(1:26, 1:12, 1:12), mse = c(pca_mse, autoencoder_mse, vae_mse), method = c(rep("PCA", 26), rep("Autoencoder", 12),rep("VAE", 12)))
ggplot(df, aes(x = k, y = mse, col = method))+ 
  geom_line()+
  labs(x = "Latent Space Dimension (k)", y = "MSE", title = "Dimension Reduction Techniques MSE")+
  theme_minimal()

```

# Customer Segements

## K-Means Clustering

### Building Customer Segments

### PCA Customer Segments

```{r, echo=FALSE}
# Load necessary library
library(stats)


# Determine optimal number of clusters using Elbow Method
set.seed(10032024)
wcss <- numeric()
for (i in 1:10) {
  kmeans_result <- kmeans(pca_scores[,14], centers = i, nstart = 25)
  wcss[i] <- kmeans_result$tot.withinss
}

# # Plotting the Elbow Plot
# plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "WCSS",
#      main = "Elbow Method for Determining Optimal Number of Clusters")

wcss_df <- data.frame(Clusters = 1:10, WCSS = wcss)

ggplot(wcss_df, aes(x = Clusters, y = WCSS)) +
  geom_line() + 
  geom_point() +
  scale_x_continuous(breaks = 1:10) + # Ensure integer scale for clusters
  theme_minimal() +
  labs(title = "Elbow Method for Determining Optimal Number of Clusters - PCA",
       x = "Number of Clusters",
       y = "Within-Cluster Sum of Squares (WCSS)")

# From the elbow we choose 3 clusters.

kmeans_result <- kmeans(pca_scores[,14], centers = 3, nstart = 25)

# credit_data_pca_segmentation = training_data_smote%>%
#                                mutate(Cluster = as.factor(kmeans_result$cluster))
# 
# cluster_means <- credit_data_pca_segmentation %>% 
#                  group_by(Cluster) %>%
#                  summarise_all(funs(mean(. , na.rm = TRUE)))
# 
# ggplot(cluster_means, aes(x = Cluster, y = LIMIT_BAL, fill = Cluster)) +
#   geom_bar(stat = "identity") +
#   theme_minimal() +
#   labs(title = "Average Credit Limit by Cluster", x = "Cluster", y = "Average Credit Limit")

credit_data_pca_segmentation <- training_data_smote %>%
  mutate(
    # Calculate credit utilization
    Avg_Utilization = rowMeans(select(., starts_with("BILL_AMT")) / LIMIT_BAL),
    # Count the number of late payments
    Num_Late_Payments = select(., starts_with("PAY_")) %>% apply(1, function(x) sum(x > 0)),
    # Determine risk level
    Risk_Level = case_when(
      Num_Late_Payments == 0 & Avg_Utilization <= 0.30 ~ "Low Risk",
      Num_Late_Payments <= 2 & Avg_Utilization > 0.30 & Avg_Utilization <= 0.75 ~ "Medium Risk",
      TRUE ~ "High Risk"
    ),
    # Add the cluster assignments
    Cluster = as.factor(kmeans_result$cluster) # This should be the cluster assignments from your k-means clustering
  )

credit_data_pca_segmentation$Risk_Level <- factor(credit_data_pca_segmentation$Risk_Level, levels = c("High Risk", "Medium Risk", "Low Risk"))

cluster_risk_summary <- credit_data_pca_segmentation %>%
  group_by(Cluster, Risk_Level) %>%
  summarise(
    Count = n(),
    Avg_Credit_Limit = mean(LIMIT_BAL, na.rm = TRUE),
    Avg_Utilization = mean(Avg_Utilization, na.rm = TRUE),
    Avg_Late_Payments = mean(Num_Late_Payments, na.rm = TRUE)
  )



risk_level_summary <- credit_data_pca_segmentation %>%
  count(Cluster, Risk_Level) %>%
  mutate(Risk_Level = factor(Risk_Level, levels = c("High Risk", "Medium Risk", "Low Risk"))) %>%
  arrange(Cluster, desc(Risk_Level)) # Ensure the order is High, Medium, Low

# Plot using ggplot2
ggplot(risk_level_summary, aes(x = Cluster, y = n, fill = Risk_Level)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Credit Risk Level Segments within Each Cluster",
       x = "Cluster",
       y = "No of customers",
       fill = "Credit Risk Level") +
  scale_fill_brewer(palette = "Set1", labels = c("High", "Medium", "Low")) # Set custom labels for the legend
```
# Model Engineering



## Linear Discriminant Analysis

## Support Vector Machines

```{r, echo=FALSE}
# objects_to_keep <- c("credit_default_data", 
#                      "training_data",
#                      "validation_data",
#                      "testing_data",
#                      "train_X_continous",
#                      "validation_X_continous",
#                      "test_X_continous",
#                      "mu_x_continous",
#                      "sigma_x_continous",
#                      "train_X_continous_oversampled",
#                      "train_X_continous_std_oversampled",
#                      "mu_x_continous_oversampled",
#                      "sigma_x_continous_oversampled",
#                      "pca_scores",
#                      "eigenvectors",
#                      "input_layer",
#                      "encoder",
#                      "autoencoder_model",
#                      "autoencoder_weights",
#                      "autoencoder_mse",
#                      "vae_mse"
#                      )
# rm(list = setdiff(ls(), objects_to_keep))
# 
# library(e1071)
# 
# svm_model = svm(DEFAULT_PAYMENT_NEXT_MONTH~ ., 
#     data = training_data,
#     type = "C-classification",
#     kernel = "linear",
#     scale = TRUE,
#     cost = 2
#     )
# 
# summary(svm_model)
# 
# 
# 
# #plot(svm_model, training_data)
# 
# svm_train_pred = predict(svm_model, training_data[, c(1:24)])
# mean(svm_train_pred==as.numeric(as.matrix(training_data[,25])))
# 
# 
# svm_train_pred = predict(svm_model, testing_data[, c(1:24)])
# mean(svm_train_pred==as.numeric(as.matrix(testing_data[,25])))

autoencoder_mse
vae_mse
```

# Model Evaluation

## Conclusion


