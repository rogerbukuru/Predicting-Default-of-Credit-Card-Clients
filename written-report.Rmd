---
title: "Predicting default of credit card clients"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: true
      df_print: kable
      fig_width: 5
      fig_height: 5
      fig_caption: true
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Import Data 

```{r, echo = FALSE}
rm(list = ls())
library(readxl)
library(tidyverse)
library(knitr)
library(ggplot2)
library(dplyr)
library(psych)

file_path = "./data/default of credit card clients.xls"

credit_default_data = as_tibble(read_xls(file_path))

colnames(credit_default_data) = as.character(credit_default_data[1, ])
                                
credit_default_data = credit_default_data[-1,]
credit_default_data = credit_default_data[,-1]

```


# Exploratory Data Analysis

## Describe Data

```{r, table_figure, fig.cap="Table 1: Example Table", echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))

credit_default_data = credit_default_data %>%
                      mutate(across(where(is.character), as.numeric))

#colnames(credit_default_data_summary)
#summary(credit_default_data)
stats = describe(credit_default_data, na.rm = FALSE, skew= FALSE,  check=TRUE, omit = TRUE)
#round(stats,2)
kable(round(stats,2), caption = "Features Summary Statistics")
```
- Education and Marriage have some unreported categories

## Response Variable Distribution

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
response_variable = as.numeric(credit_default_data$`default payment next month`) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes = response_variable %>%
              filter(Class == 1)


default_no =  response_variable %>%
              filter(Class == 0)

agg_data = response_variable %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes = nrow(default_yes)/nrow(credit_default_data) *100
percentage_no = nrow(default_no)/nrow(credit_default_data) *100  

ggplot(agg_data, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend
```


## Data Analysis and Cleaning


```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))

# Marriage and Education and to other cateogry
credit_default_data = credit_default_data %>%
                      rename(PAY_1 = PAY_0)%>% 
                      rename(DEFAULT_PAYMENT_NEXT_MONTH = `default payment next month`)

marriage_cateogory_codes = c(1,2,3)   # From data description
education_category_codes = c(1,2,3,4) # From data description

marriage_stats = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats

education_stats = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats

# Add unknown category to the 'Other' category for both Marriage and Education 
credit_default_data  = credit_default_data %>%
                       mutate(MARRIAGE = if_else(!(MARRIAGE %in% marriage_cateogory_codes), 3, MARRIAGE),
                       EDUCATION = if_else(!(EDUCATION %in% education_category_codes), 4, EDUCATION)
                       )
        

marriage_stats_2 = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats_2

education_stats_2 = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats_2


# Payment History where value is -2 and -1, aggregate under 0

payment_history = c("PAY_1", "PAY_2", "PAY_3", "PAY_4", "PAY_5","PAY_6")

for (p in payment_history) {
  indices = which(credit_default_data[,p] < 0)
  credit_default_data[indices, p] = 0
}
```


## Summary Statistics

```{r, echo = FALSE}

objects_to_keep <- c("credit_default_data")
categorical_feature_names = c("SEX", "EDUCATION", "MARRIAGE", "DEFAULT_PAYMENT_NEXT_MONTH")
credit_default_data = credit_default_data%>%
                      mutate(across(matches(categorical_feature_names), as.factor))

stats_2 = describe(credit_default_data, na.rm = FALSE, skew= FALSE, omit = TRUE)
#round(stats,2)
kable(round(stats_2,2), caption = "Features Summary Statistics")

rm(list = setdiff(ls(), objects_to_keep))
limit_balances  = credit_default_data |>
                  drop_na(LIMIT_BAL) |>
                  select(LIMIT_BAL) |>
                  as.matrix() |>
                  as.vector() |>
                  as.numeric()


average_limit_balance = mean(limit_balances)
range(limit_balances)

male_average_credit_limit = credit_default_data |>
                            filter(SEX == 1) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

female_average_credit_limit = credit_default_data |>
                            filter(SEX == 2) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

minimum = min(limit_balances)
first_quantile = quantile(limit_balances, 0.25)
median_point = quantile(limit_balances, 0.50)
third_quantile = quantile(limit_balances, 0.75)
maximum = max(limit_balances)

ggplot(credit_default_data, aes(x="", y=limit_balances)) +
  geom_boxplot(fill="lightblue", color="black") +
  labs(title="Credit Limit Five Number Summary", x="", y="Credit Limit") +
  theme_minimal()


# Credit Spending
credit_spending = credit_default_data %>%
                  select(LIMIT_BAL, BILL_AMT1) %>%
                  mutate(across(everything(), as.numeric)) %>%
                  mutate(`Credit Utilization` = BILL_AMT1) %>%
                  ungroup()
credit_spending = credit_spending %>%
                  mutate(`Percentage Utilized`= round((`Credit Utilization`/LIMIT_BAL)*100,2))


ggplot(credit_spending, aes(x = `Percentage Utilized`)) +
  geom_histogram(aes(y=..density..),binwidth = 5, # Adjust binwidth based on your data's distribution and scale
                 fill = "steelblue", color = "black") +
  xlim(0, 200)+
  labs(title = "Distribution of Credit Utilization",
       x = "Credit Utilization (%)",
       y = "# of Customers") +
  theme_minimal() 
```

## Correlation Analysis

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data",
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(corrplot)

#summary(X_categorical)

# corr_matrix = cor(train_X_continous)
# col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
# corrplot(corr_matrix, method = "color", col = col(200), 
#          type = "full", order = "original", 
#          addCoef.col = "white",
#          tl.col = "black", tl.srt = 45,
#          tl.pos = "lt",
#          tl.cex = 0.6, cl.cex = 0.7,
#          number.cex = 0.5
#          )

# Standardize Data

X_continous = credit_default_data%>%
              select(where(is.numeric))
              
corr_matrix = cor(as.matrix(X_continous))
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corr_matrix, method = "color", col = col(200),
        type = "full", order = "original",
        addCoef.col = "white",
        tl.col = "black", tl.srt = 45,
       tl.pos = "lt",
       tl.cex = 0.6, cl.cex = 0.7,
        number.cex = 0.5
       )
```

We observe from the correlation matrix, that some features have high correlations with each other

- BILL_AMT1 and BILL_AMT2 have  p = 0.95
- BILL_AMT2 and BILL_AMT3 have  p = 0.92
- BILL_AMT4 and BILL_AMT5 have  p = 0.94


# Data Preprocessing


## Categorical Features

```{r, echo = FALSE}
library(caret)
dummies = dummyVars("~.", data = credit_default_data[, -24], fullRank = FALSE)

data_transformed <- predict(dummies, newdata = credit_default_data[, -24])%>%as.tibble()
credit_default_data = data_transformed%>%
                         mutate(DEFAULT_PAYMENT_NEXT_MONTH = credit_default_data[, 24])%>%
                         select(-SEX.2, -EDUCATION.4, -MARRIAGE.3) # Remove columns to ensure full rank

```

## Dataset Partition

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", "X_continous")
rm(list = setdiff(ls(), objects_to_keep))
#summary(credit_default_data)

# Split Data
set.seed(10032024)
dataSize = nrow(credit_default_data)
trainingSize = floor(0.60*dataSize)
validationSize = floor(0.20*dataSize)


trainingDataIndices = sample(seq_len(dataSize), size = trainingSize)
remaining_indices   = c(seq_len(dataSize))[-trainingDataIndices]
validationDataIndices = sample(remaining_indices, size = validationSize)

training_data = credit_default_data[trainingDataIndices,]
validation_data = credit_default_data[validationDataIndices,]
testing_data  = credit_default_data[-c(trainingDataIndices, validationDataIndices),]

train_X_categorical = training_data %>%
              select(where(is.factor))
train_X_categorical = train_X_categorical[,-4]
train_X_continous = training_data %>%
              select(where(is.double))
train_X_continous = train_X_continous[, -1]


validation_X_categorical = validation_data %>%
              select(where(is.factor))
validation_X_categorical = validation_X_categorical[,-4]
validation_X_continous = validation_data %>%
              select(where(is.double))
validation_X_continous = validation_X_continous[, -1]


test_X_categorical = testing_data %>%
              select(where(is.factor))
test_X_categorical = test_X_categorical[,-4]
test_X_continous = testing_data %>%
              select(where(is.double))

test_X_continous = test_X_continous[, -1]

# Standardization
mu_x_continous = apply(train_X_continous, 2, mean)
sigma_x_continous = apply(train_X_continous, 2, sd)

train_X_continous_std = as.matrix(apply(train_X_continous, 2, function(train_X_continous)((train_X_continous - mean(train_X_continous))/sd(train_X_continous))) %>% as.tibble())
```


# Feature Engineering

## Feature Extraction

### Resampling

#### Oversampling

```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous"
                     )
rm(list = setdiff(ls(), objects_to_keep))
#library(smotefamily)
#install.packages(c("zoo","xts","quantmod", "ROCR")) ## and perhaps mode

#install.packages("https://cran.r-project.org/src/contrib/Archive/DMwR/DMwR_0.4.1.tar.gz", repos=NULL, type="source")
library(DMwR)

# Assume your target variable is binary and the minority class is "1"
#result <- SMOTE(form = target_variable ~ ., dat = your_data, perc.over = 100, k = 5)

train_oversampled_data <- SMOTE(DEFAULT_PAYMENT_NEXT_MONTH ~ ., data = as.data.frame(training_data), perc.over = 200, k = 5)

train_X_categorical_oversampled = train_oversampled_data %>%
              select(where(is.factor))
train_X_categorical_oversampled = train_X_categorical_oversampled[,-4]
train_X_continous_oversampled = train_oversampled_data %>%
              select(where(is.double))
train_X_continous_oversampled = train_X_continous_oversampled[, -1]

# Standardization
mu_x_continous_oversampled = apply(train_X_continous_oversampled, 2, mean)
sigma_x_continous_oversampled  = apply(train_X_continous_oversampled, 2, sd)

train_X_continous_std_oversampled  = as.matrix(apply(train_X_continous_oversampled, 2, function(train_X_continous_oversampled)((train_X_continous_oversampled - mean(train_X_continous_oversampled))/sd(train_X_continous_oversampled))) %>% as.tibble())

mu_x_continous_oversampled = apply(train_X_continous_oversampled, 2, mean)
sigma_x_continous_oversampled  = apply(train_X_continous_oversampled, 2, sd)

validation_X_continous_std  = as.matrix(apply(validation_X_continous, 2, function(validation_X_continous)((validation_X_continous - mean(validation_X_continous))/sd(validation_X_continous))) %>% as.tibble())


response_variable_2 = as.numeric(train_oversampled_data$DEFAULT_PAYMENT_NEXT_MONTH) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes_2 = response_variable_2 %>%
              filter(Class == 1)


default_no_2 =  response_variable_2 %>%
              filter(Class == 0)

agg_data_2 = response_variable_2 %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes_2 = nrow(default_yes_2)/nrow(train_oversampled_data) *100
percentage_no_2 = nrow(default_no_2)/nrow(train_oversampled_data) *100  

ggplot(agg_data_2, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend

```

### Principal Component Analysis

#### Normality Check

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "validation_X_continous_std"
                     )
rm(list = setdiff(ls(), objects_to_keep))

library(MVN)
#mvn_result <- mvn(data = train_X_continous, mvnTest = "mardia")
#print(mvn_result)

#shapiro.test(train_X_continous$LIMIT_BAL[1:5000])
```


```{r, echo= FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "validation_X_continous_std"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(ggplot2)
library(scatterplot3d)
library(plotly)
  
sigma_matrix   = cov(train_X_continous_std_oversampled)
eigen_decomp   = eigen(sigma_matrix)

eigenvalues   = eigen_decomp$values 
eigenvectors  = as.matrix(eigen_decomp$vectors)
colnames(eigenvectors) = paste("PC", 1:nrow(sigma_matrix), sep = "")


total_variance = sum(eigenvalues)
variance_explained = eigenvalues/total_variance
pca_data = data.frame(PCA = 1:nrow(sigma_matrix), `Variance Explained` = variance_explained)

# Scree Plot

ggplot(pca_data, mapping = aes(x = PCA, y = Variance.Explained)) +
      geom_point() +
      geom_line()

# Suggest that the first 4 PCA
var_first_4 = sum(variance_explained[1:4])

variance_explained

pca_scores =  t(tcrossprod(eigenvectors, train_X_continous_std_oversampled)) %>% as.tibble()
colnames(pca_scores) = colnames(eigenvectors)

#classic = prcomp(train_X_continous_std_oversampled, retx = TRUE, center = FALSE, scale. = FALSE)

#classic$rotation == eigenvectors

#classic$x
```

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "validation_X_continous_std",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))
scatterplot3d(x = pca_scores$PC1, y = pca_scores$PC2, z = pca_scores$PC3, main = "Principal Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)


```


### Autoencoders


```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "validation_X_continous_std",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))
# Autoencoder Using relu 

library(keras)


train_autoencoder = function(epochs = 100, code_size = 1) {
  
total_features = ncol(train_X_continous_std_oversampled)
input_layer = layer_input(shape = c(total_features))

encoder = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "relu") %>%# encoder layer 1
  #layer_batch_normalization() %>%  # Batch normalization layer to normalize the activations of the input layer
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = total_features/2, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = code_size) # Botteneck layer/ Laten Layer
  
decoder = 
  encoder %>%
  layer_dense(units = total_features/2 , activation = "relu") %>% # Decoder Layer 1
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = 15, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = total_features) # Reconstructed/Output layer

  
autoencoder_model = keras_model(inputs = input_layer, outputs = decoder)  
autoencoder_model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("accuracy")
)

summary(autoencoder_model)

  auto_associative =
  autoencoder_model %>%
  fit(as.matrix(train_X_continous_std_oversampled), 
             as.matrix(train_X_continous_std_oversampled),
             epochs = epochs,
             batch_size = 256,
             shuffle = TRUE,
             validation_data = list(as.matrix(validation_X_continous_std), as.matrix(validation_X_continous_std))
             )
  plot(auto_associative)
  if(code_size >= 1 & code_size <=3){
  visualize_compressed_data(autoencoder_model,input_layer, encoder, code_size)
  }
  return(list(aem = autoencoder_model))
}

visualize_compressed_data = function(autoencoder_model, input_layer, encoder, code_size = 1){
if(code_size >3){
  return(NULL)
}
autoencoder_weights = 
  autoencoder_model %>%
  get_weights()

file_name = paste("autoencoder_weights", code_size, sep = "-")
save_model_weights_hdf5(object = autoencoder_model,filepath = paste0("./data/", file_name, ".hdf5"),overwrite = TRUE)

encoder_model <- keras_model(inputs = input_layer, outputs = encoder)

encoder_model %>% load_model_weights_hdf5(filepath = paste0("./data/", file_name, ".hdf5"),skip_mismatch = TRUE,by_name = TRUE)

encoder_model %>% compile(
  loss='mean_squared_error',
  optimizer='adam',
  metrics = c('accuracy')
)

# Visualize how model has embedded data in 3-dimensions

embedded_points = encoder_model %>%
                  predict_on_batch(x = as.matrix(train_X_continous_std_oversampled)) %>%
                  as.tibble()
#plot(embedded_points$V1, embedded_points$V2)
  if(code_size == 1){
    plot(embedded_points$V1)
  }else if(code_size == 2){
        plot(embedded_points$V1, embedded_points$V2)
    
  }else if(code_size == 3){
     scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = "blue", cex.axis = 0.5)
  }else {
    print("Cannot visualize higher than 3-D")
  }
}
autoencoder_mse = rep(NA, 5)

#for(k in 1:5){
#  train_aem = train_autoencoder(epochs = 100, code_size = k)
#  autoencoder_stats = evaluate(train_aem$aem,    #as.matrix(train_X_continous_std_oversampled),as.matrix(train_X_continous_std_oversampled))
#  autoencoder_mse[k] = autoencoder_stats["loss"]
#}

#autoencoder_mse

```



```{r, echo = FALSE}

objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "validation_X_continous_std",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))



```

### Polynomial PCA

```{r, echo=FALSE}

# # Create polynomial features
# df_poly <- train_X_continous_std_oversampled %>%as.tibble()%>%
#   mutate(across(everything(), list(
#     squared = ~ .^2
#   )))
# 
# classic = prcomp(df_poly, retx = TRUE, center = FALSE, scale. = FALSE)
# 
# 
# summary(classic)
# #classic$rotation == eigenvectors
# 
# #classic$x
# 
# scatterplot3d(x = classic$rotation[,1], y = classic$rotation[,2], z = classic$rotation[,3], main = "Principal Components",xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)

```


### Variational Autoencoders

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "validation_X_continous_std",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))
sampling_layer <- new_layer_class(
  classname = "SamplingLayer",
  initialize = function(latent_dim) {
    super()$`__init__`()
    self$latent_dim <- as.integer(latent_dim)
    self$epsilon <- k_random_normal(shape = c(1, latent_dim), mean = 0, stddev = 1)  # Pre-generate noise
  },
  call = function(self, inputs) {
    z_mean <- inputs[[1]]
    z_log_var <- inputs[[2]]
    z = z_mean + k_exp(z_log_var / 2) * self$epsilon
    
    kl_loss <- -0.5 * k_sum(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1)
    kl_loss <- k_mean(kl_loss) # Take the mean of the KL divergence loss
    self$add_loss(kl_loss)
    
    return(z)
  }
)

variational_autoencoder = function(latent_space_dim,batch_size, x_train, x_validation, epochs){
  
total_features = ncol(x_train)
input_layer = layer_input(shape = c(total_features), name="inputLayer")

encoder_inputs = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "relu", name = "encoderHiddenL1")#Hidden layer 1


# Latent Space
z_mean = encoder_inputs %>% layer_dense(units = latent_space_dim, name = "z_mean")
z_log_var = encoder_inputs %>% layer_dense(units = latent_space_dim, name= "z_log_var")

sampling <- sampling_layer(latent_dim = latent_space_dim)
z <- sampling(list(z_mean, z_log_var))

encoder = keras_model(inputs = input_layer, outputs = list(z_mean, z_log_var,z), name = "encoder")
summary(encoder)


decoder_hidden_l1 = layer_dense(units = total_features/2 , activation = "relu", name="decoderHiddenL1") #Hidden layer 1
decoder_output = layer_dense(units = total_features, activation = "relu", name = "decoderOutput") # Reconstructed/Output layer

sample_decode_h_l1 = decoder_hidden_l1(z)
sample_final_decode = decoder_output(sample_decode_h_l1)
vae_model = keras_model(inputs = input_layer, outputs = sample_final_decode, name = "vae_model")
summary(vae_model)

vae_loss = function(x, vae_output){
  reconstruction_loss <- k_mean(k_square(x - vae_output), axis = -1) * latent_space_dim
  kl_loss <- -0.5 * k_sum(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1)
  k_mean(reconstruction_loss + kl_loss)
}

# Custom loss function
 reconstruction_loss <- function(x, vae_output) {
    k_mean(k_square(x - vae_output), axis = -1)
  }

vae_model %>% compile(loss = reconstruction_loss, optimizer = "adam", metrics = c("accuracy", "mse"))


vae = vae_model %>% fit(x_train, x_train,
            epochs = epochs, batch_size = batch_size,
            validation_data = list(as.matrix(x_validation), as.matrix(x_validation))
            )
plot(vae)
visual_vae_latent_data(latent_space_dim,input_layer, z, x_train, vae_model)
return(list(vae_model = vae_model))
}

visual_vae_latent_data = function(latent_space_dim,input_layer, z, x_train, vae_model){
  if(latent_space_dim >3){
  return(NULL)
}
  vae_weights = 
  vae_model %>%
  get_weights()
  
  file_name = paste("vae_weights", latent_space_dim, sep = "-")
  save_model_weights_hdf5(object = vae_model,filepath = paste0("./data/", file_name, ".hdf5"),overwrite = TRUE)
  
  encoder_model   = keras_model(inputs = input_layer, outputs = z, name = "encoder_model")
  encoder_model %>% load_model_weights_hdf5(filepath = paste0("./data/", file_name, ".hdf5"),skip_mismatch = TRUE,by_name = TRUE)
  
  embedded_points = predict(encoder_model, x_train)%>% as.tibble()
  if(latent_space_dim == 1){
    plot(embedded_points$V1)
  }else if(latent_space_dim == 2){
        plot(embedded_points$V1, embedded_points$V2)
    
  }else if(latent_space_dim == 3){
     scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Variational Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = "blue", cex.axis = 0.5)
  }else {
    print("Cannot visualize higher than 3-D")
  }
}



vae_mse = rep(NA, 5)

# for(k in 1:5){
#  train_vae_model = variational_autoencoder(
#                         latent_space_dim = k,
#                         batch_size = 256,
#                         x_train = as.matrix(train_X_continous_std_oversampled), 
#                         x_validation = as.matrix(validation_X_continous_std),
#                         epochs = 300
#                         )
#  vae_stats = evaluate(
#    train_vae_model$vae_model,
#    as.matrix(train_X_continous_std_oversampled),
#    as.matrix(train_X_continous_std_oversampled)
#    )
#  vae_mse[k] = vae_stats["loss"]
# }
# 
# vae_mse

```


### Technique evaluation

```{r, echo=FALSE}

objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_weights",
                     "autoencoder_mse",
                     "vae_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))


pca_stats = function(pca, x, x_std, mu_x, sigma_x, k){
x_projected =  x_std%*%pca[, 1:k]%>%as.tibble()
#x_projected_og_scale = sweep(x_projected,2,sigma_x, "*")
#x_projected_og_scale = sweep(x_projected,2,mu_x, "+")
mse = mean(as.matrix((as.matrix(x_std)-x_projected)^2))
return(list(x = x_projected, mse = mse))
}

pca_mse = rep(NA, 20)
for(k in 1:20){
 pca_mse[k] = pca_stats(eigenvectors,train_X_continous_oversampled, train_X_continous_std_oversampled, mu_x_continous_oversampled,     sigma_x_continous_oversampled, k)$mse

}



df <- data.frame(k = c(1:20, 1:5, 1:5), mse = c(pca_mse, autoencoder_mse, vae_mse), method = c(rep("pca", 20), rep("autoencoder", 5),rep("VAE", 5)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()

```


# Model Engineering



## Linear Discriminant Analysis

## Support Vector Machines

```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "validation_data",
                     "testing_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_weights",
                     "autoencoder_mse",
                     "vae_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))

library(e1071)

svm_model = svm(DEFAULT_PAYMENT_NEXT_MONTH~ ., 
    data = training_data,
    type = "C-classification",
    kernel = "linear",
    scale = TRUE,
    cost = 2
    )

summary(svm_model)



#plot(svm_model, training_data)

svm_train_pred = predict(svm_model, training_data[, c(1:24)])
mean(svm_train_pred==as.numeric(as.matrix(training_data[,25])))


svm_train_pred = predict(svm_model, testing_data[, c(1:24)])
mean(svm_train_pred==as.numeric(as.matrix(testing_data[,25])))


```

# Model Evaluation

## Conclusion


