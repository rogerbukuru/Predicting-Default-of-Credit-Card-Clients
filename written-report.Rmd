---
title: "Predicting default of credit card clients"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: true
      df_print: kable
      fig_width: 5
      fig_height: 5
      fig_caption: true
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Import Data 

```{r, echo = FALSE}
rm(list = ls())
library(readxl)
library(tidyverse)
library(knitr)
library(ggplot2)
library(dplyr)
library(psych)

file_path = "./data/default of credit card clients.xls"

credit_default_data = as_tibble(read_xls(file_path))

colnames(credit_default_data) = as.character(credit_default_data[1, ])
                                
credit_default_data = credit_default_data[-1,]


```


# Exploratory Data Analysis

## Feature Analysis

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))

credit_default_data = credit_default_data %>%
                      mutate(across(where(is.character), as.numeric))

colnames(credit_default_data)
#summary(credit_default_data)
stats = describe(credit_default_data, na.rm = FALSE, skew= FALSE)
round(stats,2)
```
- Education and Marriage have some unreported categories
- 
```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
response_variable = as.numeric(credit_default_data$`default payment next month`) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes = response_variable %>%
              filter(Class == 1)


default_no =  response_variable %>%
              filter(Class == 0)

agg_data = response_variable %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes = nrow(default_yes)/nrow(credit_default_data) *100
percentage_no = nrow(default_no)/nrow(credit_default_data) *100  

ggplot(agg_data, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend
```

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
limit_balances  = credit_default_data |>
                  drop_na(LIMIT_BAL) |>
                  select(LIMIT_BAL) |>
                  as.matrix() |>
                  as.vector() |>
                  as.numeric()


average_limit_balance = mean(limit_balances)
range(limit_balances)

male_average_credit_limit = credit_default_data |>
                            filter(SEX == 1) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

female_average_credit_limit = credit_default_data |>
                            filter(SEX == 2) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

minimum = min(limit_balances)
first_quantile = quantile(limit_balances, 0.25)
median_point = quantile(limit_balances, 0.50)
third_quantile = quantile(limit_balances, 0.75)
maximum = max(limit_balances)

ggplot(credit_default_data, aes(x="", y=limit_balances)) +
  geom_boxplot(fill="lightblue", color="black") +
  labs(title="Credit Limit Five Number Summary", x="", y="Credit Limit") +
  theme_minimal()
```



```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
credit_spending = credit_default_data %>%
                  select(LIMIT_BAL, BILL_AMT1) %>%
                  mutate(across(everything(), as.numeric)) %>%
                  mutate(`Credit Utilization` = BILL_AMT1) %>%
                  ungroup()
credit_spending = credit_spending %>%
                  mutate(`Percentage Utilized`= round((`Credit Utilization`/LIMIT_BAL)*100,2))


ggplot(credit_spending, aes(x = `Percentage Utilized`)) +
  geom_histogram(aes(y=..density..),binwidth = 5, # Adjust binwidth based on your data's distribution and scale
                 fill = "steelblue", color = "black") +
  xlim(0, 200)+
  labs(title = "Distribution of Credit Utilization",
       x = "Credit Utilization (%)",
       y = "# of Customers") +
  theme_minimal() 
```




## Data Cleaning


```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
# Marriage and Education and to other cateogry
credit_default_data = credit_default_data %>%
                      rename(PAY_1 = PAY_0)%>% 
                      rename(DEFAULT_PAYMENT_NEXT_MONTH = `default payment next month`)

marriage_cateogory_codes = c(1,2,3)   # From data description
education_category_codes = c(1,2,3,4) # From data description

marriage_stats = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats

education_stats = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats

# Add unknown category to the 'Other' category for both Marriage and Education 
credit_default_data  = credit_default_data %>%
                       mutate(MARRIAGE = if_else(!(MARRIAGE %in% marriage_cateogory_codes), 3, MARRIAGE),
                       EDUCATION = if_else(!(EDUCATION %in% education_category_codes), 4, EDUCATION)
                       )
        

marriage_stats_2 = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats_2

education_stats_2 = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats_2


```


```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
# Payment History where value is -2 and 0, aggregate under -1

payment_history = c("PAY_1", "PAY_2", "PAY_3", "PAY_4", "PAY_5","PAY_6")

for (p in payment_history) {
  indices = which(credit_default_data[,p] <= 0)
  credit_default_data[indices, p] = -1
}

```

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data")
rm(list = setdiff(ls(), objects_to_keep))
categorical_feature_names = c("SEX", "EDUCATION", "MARRIAGE", "DEFAULT_PAYMENT_NEXT_MONTH")
credit_default_data = credit_default_data%>%
                      mutate(across(matches(categorical_feature_names), as.factor))

#summary(credit_default_data)
# Split Data

set.seed(10032024)
dataSize = nrow(credit_default_data)
trainingSize = floor(0.60*dataSize)
validationSize = floor(0.20*dataSize)


trainingDataIndices = sample(seq_len(dataSize), size = trainingSize)
remaining_indices   = c(seq_len(dataSize))[-trainingDataIndices]
validationDataIndices = sample(remaining_indices, size = validationSize)

training_data = credit_default_data[trainingDataIndices,]
validation_data = credit_default_data[validationDataIndices,]
testing_data  = credit_default_data[-c(trainingDataIndices, validationDataIndices),]

train_X_categorical = training_data %>%
              select(where(is.factor))
train_X_categorical = train_X_categorical[,-4]
train_X_continous = training_data %>%
              select(where(is.double))
train_X_continous = train_X_continous[, -1]


validation_X_categorical = validation_data %>%
              select(where(is.factor))
validation_X_categorical = validation_X_categorical[,-4]
validation_X_continous = validation_data %>%
              select(where(is.double))
validation_X_continous = validation_X_continous[, -1]


test_X_categorical = testing_data %>%
              select(where(is.factor))
test_X_categorical = test_X_categorical[,-4]
test_X_continous = testing_data %>%
              select(where(is.double))

test_X_continous = test_X_continous[, -1]

# Standardization
mu_x_continous = apply(train_X_continous, 2, mean)
sigma_x_continous = apply(train_X_continous, 2, sd)

train_X_continous_std = as.matrix(apply(train_X_continous, 2, function(train_X_continous)((train_X_continous - mean(train_X_continous))/sd(train_X_continous))) %>% as.tibble())

```

## Correlation Analysis

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data",
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(corrplot)

#summary(X_categorical)

corr_matrix = cor(train_X_continous)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corr_matrix, method = "color", col = col(200), 
         type = "full", order = "original", 
         addCoef.col = "white",
         tl.col = "black", tl.srt = 45,
         tl.pos = "lt",
         tl.cex = 0.6, cl.cex = 0.7,
         number.cex = 0.5
         )

# Standardize Data



#corr_matrix_std = cor(X_continous_std)
#col_std <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
#corrplot(corr_matrix_std, method = "color", col_std = col(200), 
#         type = "full", order = "original", 
#         addCoef.col = "white",
#         tl.col = "black", tl.srt = 45,
#        tl.pos = "lt",
#        tl.cex = 0.6, cl.cex = 0.7,
#         number.cex = 0.5
#        )
```

We observe from the correlation matrix, that some features have high correlations with each other

- BILL_AMT1 and BILL_AMT2 have  p = 0.95
- BILL_AMT2 and BILL_AMT3 have  p = 0.92
- BILL_AMT4 and BILL_AMT5 have  p = 0.94

## Feature Selection



## Feature Extraction

### Resampling

#### Oversampling

```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous"
                     )
rm(list = setdiff(ls(), objects_to_keep))
#library(smotefamily)
#install.packages(c("zoo","xts","quantmod", "ROCR")) ## and perhaps mode

#install.packages("https://cran.r-project.org/src/contrib/Archive/DMwR/DMwR_0.4.1.tar.gz", repos=NULL, type="source")
library(DMwR)

# Assume your target variable is binary and the minority class is "1"
#result <- SMOTE(form = target_variable ~ ., dat = your_data, perc.over = 100, k = 5)

train_oversampled_data <- SMOTE(DEFAULT_PAYMENT_NEXT_MONTH ~ ., data = as.data.frame(training_data), perc.over = 200, k = 5)

train_X_categorical_oversampled = train_oversampled_data %>%
              select(where(is.factor))
train_X_categorical_oversampled = train_X_categorical_oversampled[,-4]
train_X_continous_oversampled = train_oversampled_data %>%
              select(where(is.double))
train_X_continous_oversampled = train_X_continous_oversampled[, -1]

# Standardization
mu_x_continous_oversampled = apply(train_X_continous_oversampled, 2, mean)
sigma_x_continous_oversampled  = apply(train_X_continous_oversampled, 2, sd)

train_X_continous_std_oversampled  = as.matrix(apply(train_X_continous_oversampled, 2, function(train_X_continous_oversampled)((train_X_continous_oversampled - mean(train_X_continous_oversampled))/sd(train_X_continous_oversampled))) %>% as.tibble())


response_variable_2 = as.numeric(train_oversampled_data$DEFAULT_PAYMENT_NEXT_MONTH) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes_2 = response_variable_2 %>%
              filter(Class == 1)


default_no_2 =  response_variable_2 %>%
              filter(Class == 0)

agg_data_2 = response_variable_2 %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes_2 = nrow(default_yes_2)/nrow(train_oversampled_data) *100
percentage_no_2 = nrow(default_no_2)/nrow(train_oversampled_data) *100  

ggplot(agg_data_2, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend

```

### Principal Component Analysis

```{r, echo= FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled"
                     )
rm(list = setdiff(ls(), objects_to_keep))
library(ggplot2)
library(scatterplot3d)
library(plotly)
  
sigma_matrix   = cov(train_X_continous_std_oversampled)
eigen_decomp   = eigen(sigma_matrix)

eigenvalues   = eigen_decomp$values 
eigenvectors  = as.matrix(eigen_decomp$vectors)
colnames(eigenvectors) = paste("PC", 1:nrow(sigma_matrix), sep = "")


total_variance = sum(eigenvalues)
variance_explained = eigenvalues/total_variance
pca_data = data.frame(PCA = 1:nrow(sigma_matrix), `Variance Explained` = variance_explained)

# Scree Plot

ggplot(pca_data, mapping = aes(x = PCA, y = Variance.Explained)) +
      geom_point() +
      geom_line()

# Suggest that the first 4 PCA
var_first_4 = sum(variance_explained[1:4])

variance_explained

pca_scores =  t(tcrossprod(eigenvectors, train_X_continous_std_oversampled)) %>% as.tibble()
colnames(pca_scores) = colnames(eigenvectors)

classic = prcomp(train_X_continous_std_oversampled, retx = TRUE, center = FALSE, scale. = FALSE)

classic$rotation == eigenvectors

classic$x
```

```{r, echo = FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))
scatterplot3d(x = pca_scores$PC1, y = pca_scores$PC2, z = pca_scores$PC3, main = "Principal Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)


```


### Autoencoders


```{r, echo=FALSE}
objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors"
                     )
rm(list = setdiff(ls(), objects_to_keep))
# Autoencoder Using relu 

library(keras)

total_features = ncol(train_X_continous_oversampled)
input_layer = layer_input(shape = c(total_features))

encoder = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "relu") %>%#Hidden layer 1
  layer_batch_normalization() %>%  # Batch normalization layer to normalize the activations of the input layer
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = total_features/2, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = 3, activation = "relu") # Botteneck layer/ compression and output
  
decoder = 
  encoder %>%
  layer_dense(units = total_features/2 , activation = "relu") %>% #Hidden layer 1
  #layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  #layer_dense(units = 15, activation = "relu") %>%#Hidden layer 2
  layer_dense(units = total_features, activation = "relu") # Reconstructed/Output layer

  
autoencoder_model = keras_model(inputs = input_layer, outputs = decoder)  
autoencoder_model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("accuracy")
)

summary(autoencoder_model)



train_autoencoder = function(epochs = 100) {
  auto_associative <-
  autoencoder_model %>%
  fit(as.matrix(train_X_continous_oversampled), 
             as.matrix(train_X_continous_oversampled),
             epochs = 100,
             shuffle = TRUE,
             validation_data = list(as.matrix(validation_X_continous), as.matrix(validation_X_continous))
             )
  plot(auto_associative)
}

autoencoder_mse = rep(NA, 5)
for(k in 1:5){
  #train_autoencoder(epochs = 100)
  autoencoder_stats = evaluate(autoencoder_model, train_X_continous_oversampled,train_X_continous_oversampled)
  autoencoder_mse[k] = autoencoder_stats["loss"]
}

autoencoder_mse

```



```{r, echo = FALSE}

objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))

autoencoder_weights = 
  autoencoder_model %>%
  get_weights()


save_model_weights_hdf5(object = autoencoder_model,filepath = "./data/autoencoder_weights.hdf5",overwrite = TRUE)

encoder_model <- keras_model(inputs = input_layer, outputs = encoder)

encoder_model %>% load_model_weights_hdf5(filepath = "./data/autoencoder_weights.hdf5",skip_mismatch = TRUE,by_name = TRUE)

encoder_model %>% compile(
  loss='mean_squared_error',
  optimizer='adam',
  metrics = c('accuracy')
)

# Visualize how model has embedded data in 3-dimensions

embedded_points = encoder_model %>%
                  predict_on_batch(x = as.matrix(train_X_continous_std_oversampled)) %>%
                  as.tibble()
#plot(embedded_points$V1, embedded_points$V2)
scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = "blue", cex.axis = 0.5)

```

### Variational Autoencoders

### Technique evaluation

```{r, echo=FALSE}

objects_to_keep <- c("credit_default_data", 
                     "training_data",
                     "train_X_continous",
                     "validation_X_continous",
                     "test_X_continous",
                     "mu_x_continous",
                     "sigma_x_continous",
                     "train_X_continous_oversampled",
                     "train_X_continous_std_oversampled",
                     "mu_x_continous_oversampled",
                     "sigma_x_continous_oversampled",
                     "pca_scores",
                     "eigenvectors",
                     "input_layer",
                     "encoder",
                     "autoencoder_model",
                     "autoencoder_weights",
                     "autoencoder_mse"
                     )
rm(list = setdiff(ls(), objects_to_keep))


pca_stats = function(pca, x, x_std, mu_x, sigma_x, k){
x_projected =  x_std%*%pca[, 1:k]%>%as.tibble()
#x_projected_og_scale = sweep(x_projected,2,sigma_x, "*")
#x_projected_og_scale = sweep(x_projected,2,mu_x, "+")
mse = mean(as.matrix((as.matrix(x_std)-x_projected)^2))
return(list(x = x_projected, mse = mse))
}

pca_mse = rep(NA, 20)
for(k in 1:20){
 pca_mse[k] = pca_stats(eigenvectors,train_X_continous_oversampled, train_X_continous_std_oversampled, mu_x_continous_oversampled,     sigma_x_continous_oversampled, k)$mse

}



df <- data.frame(k = c(1:20, 1:5), mse = c(pca_mse, autoencoder_mse), method = c(rep("pca", 20), rep("autoencoder", 5)))
ggplot(df, aes(x = k, y = mse, col = method)) + geom_line()

```


# Model Engineering

## Linear Discriminant Analysis

## Support Vector Machines

# Model Evaluation

## Conclusion


