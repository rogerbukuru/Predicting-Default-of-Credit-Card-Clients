---
title: "Predicting default of credit card clients"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: true
      df_print: kable
      fig_width: 5
      fig_height: 5
      fig_caption: true
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Import Data 

```{r, echo = FALSE}

rm(list = ls())
library(readxl)
library(tidyverse)
library(knitr)
library(ggplot2)
library(dplyr)
library(psych)

file_path = "./data/default of credit card clients.xls"

credit_default_data = as_tibble(read_xls(file_path))

colnames(credit_default_data) = as.character(credit_default_data[1, ])
                                
credit_default_data = credit_default_data[-1,]


```


# Exploratory Data Analysis

## Feature Analysis

```{r, echo = FALSE}

credit_default_data = credit_default_data %>%
                      mutate(across(where(is.character), as.numeric))

colnames(credit_default_data)
#summary(credit_default_data)
stats = describe(credit_default_data, na.rm = FALSE, skew= FALSE)
round(stats,2)
```
- Education and Marriage have some unreported categories
- 
```{r, echo = FALSE}
response_variable = as.numeric(credit_default_data$`default payment next month`) %>%
                    as.tibble() %>%
                    rename(., Class = value)


default_yes = response_variable %>%
              filter(Class == 1)


default_no =  response_variable %>%
              filter(Class == 0)

agg_data = response_variable %>%
           group_by(Class) %>%
           summarise(Count = n()) %>%
           mutate(Percentage = Count /sum(Count) *100)

percentage_yes = nrow(default_yes)/nrow(credit_default_data) *100
percentage_no = nrow(default_no)/nrow(credit_default_data) *100  

ggplot(agg_data, aes(x = factor(Class, labels = c("No", "Yes")), y = Count, fill = factor(Class, labels = c("No", "Yes")))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = -0.5) +
  labs(x = "Default payment next month", y = "Number of Clients", title = "Class Distribution") +
  scale_fill_manual(values = c("No" = "darkgreen", "Yes" = "red")) +  # Customize bar colors
  theme_minimal()+
  theme(legend.position = "none")  # Remove the legend
```

```{r, echo = FALSE}
limit_balances  = credit_default_data |>
                  drop_na(LIMIT_BAL) |>
                  select(LIMIT_BAL) |>
                  as.matrix() |>
                  as.vector() |>
                  as.numeric()


average_limit_balance = mean(limit_balances)
range(limit_balances)

male_average_credit_limit = credit_default_data |>
                            filter(SEX == 1) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

female_average_credit_limit = credit_default_data |>
                            filter(SEX == 2) |>
                            select(LIMIT_BAL) |>
                            as.matrix() |>
                            as.vector() |>
                            as.numeric() |>
                            mean()

minimum = min(limit_balances)
first_quantile = quantile(limit_balances, 0.25)
median_point = quantile(limit_balances, 0.50)
third_quantile = quantile(limit_balances, 0.75)
maximum = max(limit_balances)

ggplot(credit_default_data, aes(x="", y=limit_balances)) +
  geom_boxplot(fill="lightblue", color="black") +
  labs(title="Credit Limit Five Number Summary", x="", y="Credit Limit") +
  theme_minimal()
```



```{r, echo = FALSE}
credit_spending = credit_default_data %>%
                  select(LIMIT_BAL, BILL_AMT1) %>%
                  mutate(across(everything(), as.numeric)) %>%
                  mutate(`Credit Utilization` = BILL_AMT1) %>%
                  ungroup()
credit_spending = credit_spending %>%
                  mutate(`Percentage Utilized`= round((`Credit Utilization`/LIMIT_BAL)*100,2))


ggplot(credit_spending, aes(x = `Percentage Utilized`)) +
  geom_histogram(aes(y=..density..),binwidth = 5, # Adjust binwidth based on your data's distribution and scale
                 fill = "steelblue", color = "black") +
  xlim(0, 200)+
  labs(title = "Distribution of Credit Utilization",
       x = "Credit Utilization (%)",
       y = "# of Customers") +
  theme_minimal() 
```




## Data Cleaning


```{r, echo = FALSE}

# Marriage and Education and to other cateogry
credit_default_data = credit_default_data %>%
                      rename(PAY_1 = PAY_0)%>% 
                      rename(DEFAULT_PAYMENT_NEXT_MONTH = `default payment next month`)

marriage_cateogory_codes = c(1,2,3)   # From data description
education_category_codes = c(1,2,3,4) # From data description

marriage_stats = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats

education_stats = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats

# Add unknown category to the 'Other' category for both Marriage and Education 
credit_default_data  = credit_default_data %>%
                       mutate(MARRIAGE = if_else(!(MARRIAGE %in% marriage_cateogory_codes), 3, MARRIAGE),
                       EDUCATION = if_else(!(EDUCATION %in% education_category_codes), 4, EDUCATION)
                       )
        

marriage_stats_2 = credit_default_data%>%
                 group_by(MARRIAGE)%>%
                 summarise(Total = n())

marriage_stats_2

education_stats_2 = credit_default_data%>%
                 group_by(EDUCATION)%>%
                 summarise(Total = n())

education_stats_2


```


```{r, echo = FALSE}

# Payment History where value is -2 and 0, aggregate under -1

payment_history = c("PAY_1", "PAY_2", "PAY_3", "PAY_4", "PAY_5","PAY_6")

for (p in payment_history) {
  indices = which(credit_default_data[,p] <= 0)
  credit_default_data[indices, p] = -1
}

```

```{r, echo = FALSE}
#payment_history_categorical = paste("PAY_", 1:6, sep = "")
categorical_feature_names = c("SEX", "EDUCATION", "MARRIAGE", "DEFAULT_PAYMENT_NEXT_MONTH")
credit_default_data = credit_default_data%>%
                      mutate(across(matches(categorical_feature_names), as.factor))

#summary(credit_default_data)
# Split Data

set.seed(10032024)
dataSize = nrow(credit_default_data)
trainingSize = floor(0.60*dataSize)
validationSize = floor(0.20*dataSize)


trainingDataIndices = sample(seq_len(dataSize), size = trainingSize)
remaining_indices   = dataSize[-trainingDataIndices]
validationDataIndices = sample(remaining_indices, size = validationSize)

training_data = credit_default_data[trainingDataIndices,]
validation_data = credit_default_data[validationDataIndices,]
testing_data  = credit_default_data[-c(trainingDataIndices, validationDataIndices),]

```

## Correlation Analysis

```{r, echo = FALSE}
library(corrplot)

train_X_categorical = training_data %>%
              select(where(is.factor))
train_X_categorical = train_X_categorical[,-4]
train_X_continous = training_data %>%
              select(where(is.double))

train_X_continous = train_X_continous[, -1]


test_X_categorical = testing_data %>%
              select(where(is.factor))
test_X_categorical = test_X_categorical[,-4]
test_X_continous = testing_data %>%
              select(where(is.double))

test_X_continous = test_X_continous[, -1]

#summary(X_categorical)

corr_matrix = cor(train_X_continous)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(corr_matrix, method = "color", col = col(200), 
         type = "full", order = "original", 
         addCoef.col = "white",
         tl.col = "black", tl.srt = 45,
         tl.pos = "lt",
         tl.cex = 0.6, cl.cex = 0.7,
         number.cex = 0.5
         )

# Standardize Data

# Standardization
mu_x_continous = apply(train_X_continous, 2, mean)
sigma_x_continous = apply(train_X_continous, 2, sd)

train_X_continous_std = as.matrix(apply(train_X_continous, 2, function(train_X_continous)((train_X_continous - mean(train_X_continous))/sd(train_X_continous))) %>% as.tibble())

#corr_matrix_std = cor(X_continous_std)
#col_std <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
#corrplot(corr_matrix_std, method = "color", col_std = col(200), 
#         type = "full", order = "original", 
#         addCoef.col = "white",
#         tl.col = "black", tl.srt = 45,
#        tl.pos = "lt",
#        tl.cex = 0.6, cl.cex = 0.7,
#         number.cex = 0.5
#        )
```

We observe from the correlation matrix, that some features have high correlations with each other

- BILL_AMT1 and BILL_AMT2 have  p = 0.95
- BILL_AMT2 and BILL_AMT3 have  p = 0.92
- BILL_AMT4 and BILL_AMT5 have  p = 0.94

## Feature Selection



## Feature Extraction

### Principal Component Analysis

```{r, echo= FALSE}

library(ggplot2)
library(scatterplot3d)
library(plotly)
  
sigma_matrix   = cov(train_X_continous_std)
eigen_decomp   = eigen(sigma_matrix)

eigenvalues   = eigen_decomp$values 
eigenvectors  = as.matrix(eigen_decomp$vectors)
colnames(eigenvectors) = paste("PC", 1:nrow(sigma_matrix), sep = "")


total_variance = sum(eigenvalues)
variance_explained = eigenvalues/total_variance
pca_data = data.frame(PCA = 1:nrow(sigma_matrix), `Variance Explained` = variance_explained)

# Scree Plot

ggplot(pca_data, mapping = aes(x = PCA, y = Variance.Explained)) +
      geom_point() +
      geom_line()

# Suggest that the first 4 PCA
var_first_4 = sum(variance_explained[1:4])

variance_explained

pca_scores =  t(tcrossprod(eigenvectors, X_continous_std)) %>% as.tibble()
colnames(pca_scores) = colnames(eigenvectors)

```

```{r, echo = FALSE}
scatterplot3d(x = pca_scores$PC1, y = pca_scores$PC2, z = pca_scores$PC3, main = "Principal Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)

#fig <- plot_ly(pca_scores, x = ~PC1, y = ~PC2, z = ~PC3, type = 'scatter3d', mode = 'markers',
#               marker = list(size = 5, color = 'blue'))

# Customize the layout
#fig <- fig %>% layout(title = '3D Scatter Plot',
#                      scene = list(xaxis = list(title = 'PC1'),
#                                   yaxis = list(title = 'PC2'),
#                                   zaxis = list(title = 'PC3')))
#fig
#svd_decomp = svd(continous_features)

#continous_vars = credit_default_data%>%
 #                select(where(is.))

#classic_pca = prcomp(X_continous_c, retx = TRUE, center = FALSE, scale. = FALSE)
#pca_scores_two = classic_pca$x %>% as.tibble()

#scatterplot3d(x = pca_scores_two$PC1, y = pca_scores_two$PC2, z = pca_scores_two$PC3, main = "Principal Components",
             # xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue")

#classic_pca$x == pca_scores
```


### Autoencoders

```{r, echo = FALSE}
#install.packages("h2o", type = "source", repos = (c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))

library(h2o)
library(scatterplot3d)
library(ggplot2)
h2o.init()
features <- as.h2o(X_continous_std)

# Train an autoencoder
ae1 <- h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 3,
  activation = 'Tanh',
  sparse = FALSE
)

ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings

codings_df <- as.data.frame(ae1_codings)

ggplot(codings_df, aes(x = codings_df$DF.L1.C1, y = codings_df$DF.L1.C2)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Autoencoder Components",
       x = "Component 1",
       y = "Component 2")

scatterplot3d(x = codings_df$DF.L1.C1, y = codings_df$DF.L1.C2, z = codings_df$DF.L1.C3, main = "Autoencoder Components",
              xlab = "PC1", ylab = "PC2", zlab = "PC3", pch = 19, color = "blue", cex.axis = 0.5)
```


```{r, echo=FALSE}
# autoencoder in keras
library(keras)

total_features = ncol(train_X_continous)
input_layer = layer_input(shape = c(total_features))

encoder = 
  input_layer %>%
  layer_dense(units = total_features/2, activation = "tanh") %>%#Hidden layer 1
  layer_batch_normalization() %>%  # Batch normalization layer to normalize the activations of the input layer
  layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  layer_dense(units = 3, activation = "tanh") # Botteneck layer/ compression and output
  
decoder = 
  encoder %>%
  layer_dense(units = total_features/2, activation = "tanh") %>% #Hidden layer 1
  layer_dropout(rate = 0.1) %>%  #Dropout layer to prevent overfitting by randomly dropping 10% of the input units.
  layer_dense(units = total_features, activation = "tanh") # Reconstructed/Output layer

  
autoencoder_model = keras_model(inputs = input_layer, outputs = decoder)  
autoencoder_model %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = c("accuracy")
)

summary(autoencoder_model)
  
train_autoencoder = function(epochs = 100) {
  auto_associative <-
  autoencoder_model %>%
  keras::fit(as.matrix(train_X_continous), 
             as.matrix(train_X_continous),
             epochs = 100,
             shuffle = TRUE,
             validation_data = list(as.matrix(test_X_continous), as.matrix(test_X_continous))
             )
  plot(auto_associative)
}

#train_autoencoder(epochs = 100)

```



```{r, echo = FALSE}

autoencoder_weights = 
  autoencoder_model %>%
  keras::get_weights()


keras::save_model_weights_hdf5(object = autoencoder_model,filepath = "./data/autoencoder_weights.hdf5",overwrite = TRUE)

encoder_model <- keras_model(inputs = input_layer, outputs = encoder)

encoder_model %>% keras::load_model_weights_hdf5(filepath = "./data/autoencoder_weights.hdf5",skip_mismatch = TRUE,by_name = TRUE)

encoder_model %>% compile(
  loss='mean_squared_error',
  optimizer='adam',
  metrics = c('accuracy')
)

# Visualize how model has embedded data in 3-dimensions

embedded_points = encoder_model %>%
                  keras::predict_on_batch(x = as.matrix(train_X_continous)) %>%
                  as.tibble()

scatterplot3d(x = embedded_points$V1, y = embedded_points$V2, z = embedded_points$V3, main = "Autoencoder 3D Embedding", xlab = "Dim 1", ylab = "Dim 2", zlab = "Dim 3", pch = 19, color = "blue", cex.axis = 0.5)

                  
```
# Model Engineering

# Model Evaluation

## Conclusion


